{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Days On Market Feature Selection\n",
    "\n",
    "## Information\n",
    "\n",
    "Housing related data sources were combined in the project SQLite database. The output CSV file is analyzed here. \n",
    "\n",
    "### Environment Information:\n",
    "\n",
    "Environment used for coding is as follow:\n",
    "\n",
    "Oracle VM VirtualBox running Ubuntu (guest) on Windows 10 (host).\n",
    "\n",
    "Current conda install:\n",
    "\n",
    "               platform : linux-64\n",
    "          conda version : 4.2.13\n",
    "       conda is private : False\n",
    "      conda-env version : 4.2.13\n",
    "    conda-build version : 1.20.0\n",
    "         python version : 2.7.11.final.0\n",
    "       requests version : 2.9.1\n",
    "       \n",
    "Package requirements:\n",
    "\n",
    "dill : 0.2.4, numpy : 1.11.2, pandas : 0.19.1, matplotlib : 1.5.1, scipy : 0.18.1, seaborn : 0.7.0, scikit-image : 0.12.3, scikit-learn : 0.18.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Package(s) Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score   \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data csv into dataframe\n",
    "df = pd.read_csv('df_prep_for_feature_selection_output.csv')\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performing feature selection on full dataset, resulted in best_score ~ 0.1.\n",
    "# Qcutting to separate out data.\n",
    "# qcut value = 1 is dataset as is.\n",
    "\n",
    "# Qcut DOMP target data\n",
    "df_2['qcut_DOMP'] = pd.qcut(df_2['DOMP'], 1, labels = False)\n",
    "# Print out total row counts for each group\n",
    "print(df_2['qcut_DOMP'].value_counts())\n",
    "# Select specific range\n",
    "df_2 = df_2[df_2['qcut_DOMP'] == 0]\n",
    "# Save dataframe to disk\n",
    "df_2.to_csv('df_feature_selection_save_point.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy dataframe for dropping columns and determining categorical columns\n",
    "df_3 = df_2.copy()\n",
    "# Drop target column and qcut column for test-train-split\n",
    "df_3 = df_3.drop('DOMP', axis=1)\n",
    "df_3 = df_3.drop('qcut_DOMP', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking for grouped categorical columns\n",
    "#df_3.columns[93:178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns that are not scaled since they are categorical\n",
    "# cat_df = df_3[[u'BasementY/N']]\n",
    "# cat_df_2 = df_3.ix[:,39:43]\n",
    "# cat_df_3 = df_3.ix[:,48:52]\n",
    "# cat_df_4 = df_3.ix[:,57:61]\n",
    "# cat_df_5 = df_3.ix[:,93:178]\n",
    "# cat_df_6 = pd.concat([cat_df,cat_df_2,cat_df_3,cat_df_4,cat_df_5],axis=1)\n",
    "# CATEGORICAL = [x for x in cat_df_6.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CATEGORICAL = ['BasementY/N','ES_IsCharter','ES_IsMagnet','ES_IsTitleI','ES_IsVirtual',\n",
    "               'HS_IsCharter','HS_IsMagnet','HS_IsTitleI','HS_IsVirtual','MS_IsCharter',\n",
    "               'MS_IsMagnet','MS_IsTitleI','MS_IsVirtual','zip_20001','zip_20002','zip_20004',\n",
    "               'zip_20005','zip_20007','zip_20008','zip_20009','zip_20010','zip_20011',\n",
    "               'zip_20012','zip_20015','zip_20017','zip_20018','zip_20019','zip_20020',\n",
    "               'zip_20032','zip_20036','zip_20037','ldmonth_1','ldmonth_2','ldmonth_3',\n",
    "               'ldmonth_4','ldmonth_5','ldmonth_6','ldmonth_7','ldmonth_8','ldmonth_9',\n",
    "               'ldmonth_10','ldmonth_11','ldmonth_12','ldday_1','ldday_2','ldday_3',\n",
    "               'ldday_4','ldday_5','ldday_6','ldday_7','ldday_8','ldday_9','ldday_10',\n",
    "               'ldday_11','ldday_12','ldday_13','ldday_14','ldday_15','ldday_16','ldday_17',\n",
    "               'ldday_18','ldday_19','ldday_20','ldday_21','ldday_22','ldday_23','ldday_24',\n",
    "               'ldday_25','ldday_26','ldday_27','ldday_28','ldday_29','ldday_30','ldday_31',\n",
    "               'ESSR_0.0','ESSR_1.0','ESSR_2.0','ESSR_3.0','ESSR_4.0','ESSR_5.0','HSSR_0.0',\n",
    "               'HSSR_1.0','HSSR_2.0','HSSR_3.0','HSSR_4.0','MSSR_0.0','MSSR_1.0','MSSR_2.0',\n",
    "               'MSSR_3.0','MSSR_4.0','MSSR_5.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV parameters for pipeline\n",
    "# MAE not used for rfr,etr,or gbr. Program would crash or hit memory limitations \n",
    "# if used with MAE.\n",
    "rfr_etr_params = {'estimator__n_estimators':[100],'estimator__criterion':['mse'],\n",
    "                   'estimator__max_features':['auto'],'estimator__min_samples_leaf':[1,2,5],\n",
    "                   'estimator__random_state':[1]}\n",
    "\n",
    "gbr_params = {'estimator__loss':['ls','lad','huber'], \n",
    "              'estimator__learning_rate':[0.1],\n",
    "              'estimator__n_estimators':[100],\n",
    "              'estimator__criterion':['friedman_mse','mse'],\n",
    "              'estimator__min_samples_leaf':[1,2,5],\n",
    "              'estimator__max_features':['auto'],    \n",
    "              'estimator__random_state':[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_feature_selection(df,feature_model,feature_model_str,model_estimator,model_estimator_str,pipe_params,threshold_num,threshold_str,CATEGORICAL):\n",
    "\n",
    "    \"\"\"\n",
    "    Test different thresholds of feature importance for feature selection, using forest-based\n",
    "    ensemble models. The pipeline test-train-splits the dataset, and performs modelling as a\n",
    "    function of feature importance threshold, using SelectFromModel. Individual models are\n",
    "    saved as dills, and the regression metrics are saved to csv.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start clock for run time\n",
    "    start  = time.time()\n",
    "    \n",
    "    # Copy dataframe\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    # Drop target column and qcut column for test-train-split\n",
    "    df_2 = df_2.drop('DOMP', axis=1)\n",
    "    df_2 = df_2.drop('qcut_DOMP', axis=1)\n",
    "    \n",
    "    # Test-train split. Using 70/30% split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_2, df['DOMP'], train_size=0.70,\n",
    "                                                    random_state=1)    \n",
    "    \n",
    "    # Standardizing training and testing data. Standardized separately to avoid information\n",
    "    # leaking from the training set to the testing set. Categorical data not scaled.\n",
    "    for i in X_train.columns.difference(CATEGORICAL):\n",
    "        X_train[i] = StandardScaler().fit_transform(X_train[i].values.reshape(-1,1))\n",
    "\n",
    "    for i in X_test.columns.difference(CATEGORICAL):\n",
    "        X_test[i] = StandardScaler().fit_transform(X_test[i].values.reshape(-1,1))\n",
    "    \n",
    "    # Pipeline using coefficient- or feature importances-base estimator with SelectFromModel\n",
    "    pipe = Pipeline([('feature_selection', SelectFromModel(feature_model, threshold=threshold_num)),\n",
    "                      ('estimator', model_estimator)\n",
    "                     ])\n",
    "        \n",
    "    # GridSearchCV for pipeline\n",
    "    grid = GridSearchCV(pipe, pipe_params, cv = 12, n_jobs = -1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Print out best score and respective parameters\n",
    "    print 'Best score from GridSearchCV Pipeline for threshold '+threshold_str+' is ',grid.best_score_\n",
    "    #print \"Best model parameters from GridSearch are \",grid.best_estimator_.get_params()\n",
    "    \n",
    "    # Save model to disk\n",
    "    dill.dump(grid, open('SFM_feature_selection_'+feature_model_str+'_threshold_'+threshold_str+'_model_estimator_'+model_estimator_str, 'wb'))\n",
    "    dill.dump(grid.best_estimator_, open('SFM_feature_selection_'+feature_model_str+'_threshold_'+threshold_str+'_best_model_estimator_'+model_estimator_str, 'wb'))\n",
    "\n",
    "    # Predicted target values\n",
    "    y_pred = grid.predict(X_test)\n",
    "   \n",
    "    # Store regression metrics\n",
    "    exp_var_score = explained_variance_score(y_test, y_pred)\n",
    "    #r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)    \n",
    "\n",
    "    # Create dataframe to save regression metrics\n",
    "    df_combination = pd.DataFrame(columns = {'threshold','r2_score','exp_var_score',\n",
    "                                             'mae','mse','process_time'})\n",
    "    threshold_list_lst = []\n",
    "    exp_var_score_lst = []\n",
    "    r2_score_lst = []\n",
    "    mae_lst = []\n",
    "    mse_lst = []\n",
    "    process_time_lst = []\n",
    "    \n",
    "    # Save metrics to separate lists for inclusion in dataframe. Saving directly to dataframe\n",
    "    # resulted in typeerrors being flagged.\n",
    "    threshold_list_lst.append(threshold_num)\n",
    "    exp_var_score_lst.append(exp_var_score)\n",
    "    r2_score_lst.append(grid.best_score_) # best_score here is the r2_score, since GridSearchCV\n",
    "                                          # uses the default score metrics from the estimator \n",
    "    mae_lst.append(mae)\n",
    "    mse_lst.append(mse)    \n",
    "    process_time_lst.append(time.time()-start)\n",
    "    \n",
    "    # Add lists as series to dataframe, and save file to disk.\n",
    "    df_combination['threshold'] = threshold_list_lst\n",
    "    df_combination['exp_var_score'] = exp_var_score_lst\n",
    "    df_combination['r2_score'] = r2_score_lst\n",
    "    df_combination['mae'] = mae_lst\n",
    "    df_combination['mse'] = mse_lst\n",
    "    df_combination['process_time'] = process_time_lst\n",
    "    df_combination.to_csv('feature_selection_'+feature_model_str+'_threshold_'+threshold_str+'_model_estimator_'+model_estimator_str+'.csv')\n",
    "    \n",
    "    # Print run time\n",
    "    print \"\\nBuild and Validation took {:0.3f} seconds\\n\".format(time.time()-start)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.1,'01',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.09,'009',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.08,'008',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.07,'007',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.06,'006',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.05,'005',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.04,'004',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.03,'003',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.02,'002',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.01,'001',CATEGORICAL)\n",
    "threshold_feature_selection(df_2,LassoCV(max_iter=10000,random_state=1),'LCV',\n",
    "                            RandomForestRegressor(),'RFR',rfr_etr_params,0.00,'000',CATEGORICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot score vs threshold to determine optimum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_score_vs_threshold(feature_model_str,model_estimator_str):\n",
    "    # Import dataframes\n",
    "    \n",
    "    # For ETR and ETR, errors were flagged for importance thresholds of 0.1,0.09,and 0.08.\n",
    "    # No features were present there for the data.\n",
    "    \n",
    "    df_threshold_01 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_01_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_009 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_009_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_008 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_008_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_007 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_007_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_006 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_006_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_005 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_005_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_004 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_004_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_003 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_003_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_002 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_002_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_001 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_001_model_estimator_'+model_estimator_str+'.csv')\n",
    "    df_threshold_000 = pd.read_csv('SFM_feature_selection_'+feature_model_str+'_threshold_000_model_estimator_'+model_estimator_str+'.csv')\n",
    "    \n",
    "    # Row append dataframes\n",
    "    df_threshold = df_threshold_01.copy()\n",
    "    df_threshold = df_threshold.append([df_threshold_009,df_threshold_008,df_threshold_007,\n",
    "                         df_threshold_006,df_threshold_005,df_threshold_004,\n",
    "                         df_threshold_003,df_threshold_002,df_threshold_001,df_threshold_000]) \n",
    "    df_threshold = df_threshold.drop('Unnamed: 0', axis=1)\n",
    "    df_threshold_sorted = df_threshold.sort_values(by='threshold')\n",
    "    \n",
    "    # Save dataframe to disk\n",
    "    df_threshold_sorted.to_csv('SFM_feature_selection_'+feature_model_str+'_threshold_model_estimator_'+model_estimator_str+'_regression_metrics.csv')\n",
    "    \n",
    "    # Plotting sorted feature importance\n",
    "    plt.figure()\n",
    "    df_threshold_sorted.plot.line('threshold','r2_score',linewidth=3)\n",
    "    plt.title('r^2_score vs importance threshold',fontsize=18,fontweight='bold')\n",
    "    plt.xlabel('Threshold',fontsize=14,fontweight='bold')\n",
    "    plt.xticks(fontsize=14,fontweight='bold')\n",
    "    plt.ylabel('Coefficient of Determination (r^2)',fontsize=14,fontweight='bold')\n",
    "    plt.yticks(fontsize=14,fontweight='bold')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig('SFM_feature_selection_'+feature_model_str+'_threshold_model_estimator_'+model_estimator_str+'_plot_score_vs_important_features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_score_vs_threshold('LCV','RFR')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Days On Market PCA Feature and Model Selection\n",
    "\n",
    "## Information\n",
    "\n",
    "Housing related data sources were combined in the project SQLite database. The output CSV file is analyzed here. \n",
    "\n",
    "### Environment Information:\n",
    "\n",
    "Environment used for coding is as follow:\n",
    "\n",
    "Oracle VM VirtualBox running Ubuntu (guest) on Windows 10 (host).\n",
    "\n",
    "Current conda install:\n",
    "\n",
    "               platform : linux-64\n",
    "          conda version : 4.2.13\n",
    "       conda is private : False\n",
    "      conda-env version : 4.2.13\n",
    "    conda-build version : 1.20.0\n",
    "         python version : 2.7.11.final.0\n",
    "       requests version : 2.9.1\n",
    "       \n",
    "Package requirements:\n",
    "\n",
    "dill : 0.2.4, numpy : 1.11.2, pandas : 0.19.1, matplotlib : 1.5.1, scipy : 0.18.1, seaborn : 0.7.0, scikit-image : 0.12.3, scikit-learn : 0.18.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Package(s) Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV, SGDRegressor, HuberRegressor, PassiveAggressiveRegressor, TheilSenRegressor\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score   \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListPrice2</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>BathsFull</th>\n",
       "      <th>BathsHalf</th>\n",
       "      <th>Levels</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>BasementY/N</th>\n",
       "      <th>Acres</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>DOMP</th>\n",
       "      <th>...</th>\n",
       "      <th>HSSR_1.0</th>\n",
       "      <th>HSSR_2.0</th>\n",
       "      <th>HSSR_3.0</th>\n",
       "      <th>HSSR_4.0</th>\n",
       "      <th>MSSR_0.0</th>\n",
       "      <th>MSSR_1.0</th>\n",
       "      <th>MSSR_2.0</th>\n",
       "      <th>MSSR_3.0</th>\n",
       "      <th>MSSR_4.0</th>\n",
       "      <th>MSSR_5.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "      <td>1950</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1987</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>299900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1941</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1941</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1891</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ListPrice2  Bedrooms  BathsFull  BathsHalf  Levels  Fireplaces  \\\n",
       "0      269900         3          2        0.0       3           0   \n",
       "1      255000         2          2        1.0       2           1   \n",
       "2      299900         1          1        0.0       1           0   \n",
       "3      245000         1          1        0.0       1           0   \n",
       "4      250000         1          1        0.0       1           0   \n",
       "\n",
       "   BasementY/N  Acres  YearBuilt  DOMP    ...     HSSR_1.0  HSSR_2.0  \\\n",
       "0            1  0.082       1950    10    ...          0.0       1.0   \n",
       "1            0  0.000       1987    41    ...          0.0       1.0   \n",
       "2            0  0.000       1941    12    ...          0.0       1.0   \n",
       "3            0  0.000       1941    16    ...          0.0       1.0   \n",
       "4            0  0.000       1891     5    ...          0.0       1.0   \n",
       "\n",
       "   HSSR_3.0  HSSR_4.0  MSSR_0.0  MSSR_1.0  MSSR_2.0  MSSR_3.0  MSSR_4.0  \\\n",
       "0       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "4       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "\n",
       "   MSSR_5.0  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data csv into dataframe\n",
    "df = pd.read_csv('df_prep_for_feature_selection_output.csv')\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    13725\n",
      "Name: qcut_DOMP, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Performing feature selection on full dataset, resulted in best_score ~ 0.1.\n",
    "# Qcutting to separate out data.\n",
    "# qcut value = 1 is dataset as is.\n",
    "\n",
    "# Qcut DOMP target data\n",
    "df_2['qcut_DOMP'] = pd.qcut(df_2['DOMP'], 1, labels = False)\n",
    "# Print out total row counts for each group\n",
    "print(df_2['qcut_DOMP'].value_counts())\n",
    "# Select specific range\n",
    "df_2 = df_2[df_2['qcut_DOMP'] == 0]\n",
    "# Save dataframe to disk\n",
    "df_2.to_csv('df_feature_selection_save_point.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy dataframe for dropping columns and determining categorical columns\n",
    "df_3 = df_2.copy()\n",
    "# Drop target column and qcut column for test-train-split\n",
    "df_3 = df_3.drop('DOMP', axis=1)\n",
    "df_3 = df_3.drop('qcut_DOMP', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking for grouped categorical columns\n",
    "#df_3.columns[93:178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns that are not scaled since they are categorical\n",
    "# cat_df = df_3[[u'BasementY/N']]\n",
    "# cat_df_2 = df_3.ix[:,39:43]\n",
    "# cat_df_3 = df_3.ix[:,48:52]\n",
    "# cat_df_4 = df_3.ix[:,57:61]\n",
    "# cat_df_5 = df_3.ix[:,93:178]\n",
    "# cat_df_6 = pd.concat([cat_df,cat_df_2,cat_df_3,cat_df_4,cat_df_5],axis=1)\n",
    "# CATEGORICAL = [x for x in cat_df_6.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CATEGORICAL = ['BasementY/N','ES_IsCharter','ES_IsMagnet','ES_IsTitleI','ES_IsVirtual',\n",
    "               'HS_IsCharter','HS_IsMagnet','HS_IsTitleI','HS_IsVirtual','MS_IsCharter',\n",
    "               'MS_IsMagnet','MS_IsTitleI','MS_IsVirtual','zip_20001','zip_20002','zip_20004',\n",
    "               'zip_20005','zip_20007','zip_20008','zip_20009','zip_20010','zip_20011',\n",
    "               'zip_20012','zip_20015','zip_20017','zip_20018','zip_20019','zip_20020',\n",
    "               'zip_20032','zip_20036','zip_20037','ldmonth_1','ldmonth_2','ldmonth_3',\n",
    "               'ldmonth_4','ldmonth_5','ldmonth_6','ldmonth_7','ldmonth_8','ldmonth_9',\n",
    "               'ldmonth_10','ldmonth_11','ldmonth_12','ldday_1','ldday_2','ldday_3',\n",
    "               'ldday_4','ldday_5','ldday_6','ldday_7','ldday_8','ldday_9','ldday_10',\n",
    "               'ldday_11','ldday_12','ldday_13','ldday_14','ldday_15','ldday_16','ldday_17',\n",
    "               'ldday_18','ldday_19','ldday_20','ldday_21','ldday_22','ldday_23','ldday_24',\n",
    "               'ldday_25','ldday_26','ldday_27','ldday_28','ldday_29','ldday_30','ldday_31',\n",
    "               'ESSR_0.0','ESSR_1.0','ESSR_2.0','ESSR_3.0','ESSR_4.0','ESSR_5.0','HSSR_0.0',\n",
    "               'HSSR_1.0','HSSR_2.0','HSSR_3.0','HSSR_4.0','MSSR_0.0','MSSR_1.0','MSSR_2.0',\n",
    "               'MSSR_3.0','MSSR_4.0','MSSR_5.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV parameters for pipeline\n",
    "# MAE not used for rfr,etr,or gbr. Program would crash or hit memory limitations \n",
    "# if used with MAE.\n",
    "# Parameters for estimator GridSearch\n",
    "\n",
    "# MAE not used for rfr,etr,or gbr. Program would crash or hit memory limitations \n",
    "# if used with MSE.\n",
    "rfr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__n_estimators':[100],'estimator__criterion':['mse'],\n",
    "              'estimator__max_features':['auto'],'estimator__min_samples_leaf':[1,2,5],\n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "etr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__n_estimators':[100],'estimator__criterion':['mse'],\n",
    "              'estimator__max_features':['auto'],'estimator__min_samples_leaf':[1,2,5],\n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "gbr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__loss':['ls','lad','huber'],'estimator__learning_rate':[0.1],\n",
    "              'estimator__n_estimators':[100],'estimator__criterion':['friedman_mse','mse'],\n",
    "              'estimator__min_samples_leaf':[1,2,5],'estimator__max_features':['auto'],\n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "sgdr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "               'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "               'estimator__loss':['squared_loss','huber','epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "               'estimator__alpha':[0.0001,0.001,0.01,0.1,1.0],\n",
    "               'estimator__fit_intercept':[True,False],'estimator__n_iter':[5,10],     \n",
    "               'estimator__random_state':[1]}\n",
    "\n",
    "tsr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__fit_intercept':[True,False],'estimator__max_iter':[400],     \n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "par_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__loss':['epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "              'estimator__C':[0.001,0.01,0.1,1.0],\n",
    "              'estimator__fit_intercept':[True,False],'estimator__n_iter':[5,10],     \n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "hr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "             'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "             'estimator__alpha':[0.0001,0.001,0.01,0.1,1.0],\n",
    "             'estimator__fit_intercept':[True,False],'estimator__max_iter':[200]}\n",
    "\n",
    "lcv_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__eps':[0.001,0.01,0.1],'estimator__fit_intercept':[True,False],\n",
    "              'estimator__cv':[4],'estimator__max_iter':[25000],\n",
    "              'estimator__random_state':[1]}\n",
    "\n",
    "rcv_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__alphas':[np.array([0.1,1.0,10.0])],\n",
    "              'estimator__fit_intercept':[True,False],'estimator__cv':[4]}\n",
    "\n",
    "encv_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "               'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "               'estimator__l1_ratio':[0.2,0.4,0.6,0.8],'estimator__eps':[0.001,0.01,0.1],\n",
    "               'estimator__fit_intercept':[True,False],'estimator__cv':[4],\n",
    "               'estimator__max_iter':[25000],'estimator__random_state':[1]}\n",
    "\n",
    "knnr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "               'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "               'estimator__n_neighbors':np.arange(2,22,2),\n",
    "               'estimator__weights':['uniform','distance'],\n",
    "               'estimator__algorithm':['ball_tree','kd_tree']}\n",
    "\n",
    "svr_params = {'decomposition__n_components':np.arange(1,11,1),\n",
    "              'decomposition__whiten':[True],'decomposition__random_state':[1],\n",
    "              'estimator__C':[0.01,0.1,1.0],'estimator__kernel':['poly','rbf'],\n",
    "              'estimator__degree':[1,2,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decomposition_feature_model_selection(df,model_estimator,model_estimator_str,pipe_params,CATEGORICAL):\n",
    "\n",
    "    \"\"\"\n",
    "    The pipeline test-train-splits the dataset, and performs modelling with PCA for decomposition\n",
    "    followed by GridSearchCV. Individual models are saved as dills, and the regression metrics\n",
    "    are saved to csv.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start clock for run time\n",
    "    start  = time.time()\n",
    "    \n",
    "    # Copy dataframe\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    # Drop target column and qcut column for test-train-split\n",
    "    df_2 = df_2.drop('DOMP', axis=1)\n",
    "    df_2 = df_2.drop('qcut_DOMP', axis=1)\n",
    "    \n",
    "    # Test-train split. Using 70/30% split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_2, df['DOMP'], train_size=0.70,\n",
    "                                                    random_state=1)    \n",
    "    \n",
    "    # Standardizing training and testing data. Standardized separately to avoid information\n",
    "    # leaking from the training set to the testing set. Categorical data not scaled.\n",
    "    for i in X_train.columns.difference(CATEGORICAL):\n",
    "        X_train[i] = StandardScaler().fit_transform(X_train[i].values.reshape(-1,1))\n",
    "\n",
    "    for i in X_test.columns.difference(CATEGORICAL):\n",
    "        X_test[i] = StandardScaler().fit_transform(X_test[i].values.reshape(-1,1))\n",
    "    \n",
    "    # Pipeline using PCA\n",
    "    pipe = Pipeline([('decomposition', PCA()),\n",
    "                      ('estimator', model_estimator)\n",
    "                     ])\n",
    "        \n",
    "    # GridSearchCV for pipeline\n",
    "    grid = GridSearchCV(pipe, pipe_params, cv = 12, n_jobs = -1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Print out best score and respective parameters\n",
    "    print('Best score from GridSearchCV Pipeline for '+model_estimator_str+' is ',grid.best_score_)\n",
    "    #print \"Best model parameters from GridSearch are \",grid.best_estimator_.get_params()\n",
    "    \n",
    "    # Save model to disk\n",
    "    dill.dump(grid, open('PCA_model_selection_'+model_estimator_str+'_no prior_feature_selection', 'wb'))\n",
    "    dill.dump(grid.best_estimator_, open('PCA_best_model_selection_'+model_estimator_str+'_no_prior_feature_selection', 'wb'))\n",
    "\n",
    "    # Predicted target values\n",
    "    y_pred = grid.predict(X_test)\n",
    "   \n",
    "    # Store regression metrics\n",
    "    exp_var_score = explained_variance_score(y_test, y_pred)\n",
    "    #r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)    \n",
    "\n",
    "    # Create dataframe to save regression metrics\n",
    "    df_combination = pd.DataFrame(columns = {'estimator','r2_score','exp_var_score',\n",
    "                                             'mae','mse','process_time'})\n",
    "    estimator_list_lst = []\n",
    "    exp_var_score_lst = []\n",
    "    r2_score_lst = []\n",
    "    mae_lst = []\n",
    "    mse_lst = []\n",
    "    process_time_lst = []\n",
    "    \n",
    "    # Save metrics to separate lists for inclusion in dataframe. Saving directly to dataframe\n",
    "    # resulted in typeerrors being flagged.\n",
    "    estimator_list_lst.append(model_estimator_str)\n",
    "    exp_var_score_lst.append(exp_var_score)\n",
    "    r2_score_lst.append(grid.best_score_) # best_score here is the r2_score, since GridSearchCV\n",
    "                                          # uses the default score metrics from the estimator \n",
    "    mae_lst.append(mae)\n",
    "    mse_lst.append(mse)    \n",
    "    process_time_lst.append(time.time()-start)\n",
    "    \n",
    "    # Add lists as series to dataframe, and save file to disk.\n",
    "    df_combination['estimator'] = estimator_list_lst\n",
    "    df_combination['exp_var_score'] = exp_var_score_lst\n",
    "    df_combination['r2_score'] = r2_score_lst\n",
    "    df_combination['mae'] = mae_lst\n",
    "    df_combination['mse'] = mse_lst\n",
    "    df_combination['process_time'] = process_time_lst\n",
    "    df_combination.to_csv('PCA_model_selection_'+model_estimator_str+'_no prior_feature_selection.csv')\n",
    "        \n",
    "    # Print run time\n",
    "    print(\"\\nBuild and Validation took {:0.3f} seconds\\n\".format(time.time()-start))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposition_feature_model_selection(df_2,LassoCV(),'LCV',lcv_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,RidgeCV(),'RCV',rcv_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,ElasticNetCV(),'ENCV',encv_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,SGDRegressor(),'SGDR',sgdr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,TheilSenRegressor(),'TSR',tsr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,PassiveAggressiveRegressor(),'PAR',par_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,HuberRegressor(),'HR',hr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,RandomForestRegressor(),'RFR',rfr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,ExtraTreesRegressor(),'ETR',etr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,GradientBoostingRegressor(),'GBR',gbr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,KNeighborsRegressor(),'KNNR',knnr_params,CATEGORICAL)\n",
    "#decomposition_feature_model_selection(df_2,SVR(),'SVR',svr_params,CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_PCA_explained_variance_vs_n_components(df,model_estimator_str):\n",
    "    # Import dill file of model\n",
    "    model = dill.load(open('PCA_model_selection_'+model_estimator_str+'_no prior_feature_selection', 'rb'))\n",
    "     \n",
    "    # Plot explained variance vs. number of components\n",
    "    plt.figure()\n",
    "    plt.plot(model.named_steps['decomposition'].explained_variance_,linewidth=3)\n",
    "    plt.title('Explained variance vs number of components',fontsize=18,fontweight='bold')\n",
    "    plt.xlabel(\"Number of components\",fontsize=14,fontweight='bold')\n",
    "    plt.xticks(fontsize=14,fontweight='bold')\n",
    "    plt.ylabel(\"Explained variance\",fontsize=14,fontweight='bold')\n",
    "    plt.yticks(fontsize=14,fontweight='bold')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig('PCA_model_selection_'+model_estimator_str+'_plot_explained_variance_vs_number_of_components')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_PCA_explained_variance_vs_n_components(df_3,'LCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
